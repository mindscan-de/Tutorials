# Write up for arxiv:1801.06146 -- ULMFiT-paper

This is a write up for the 'Universal LanguageModel Fine-tuning for Text-Classification' paper of Jeremy Howard and Rebastian Ruder.

# Why

I do this hopefully short writeup, to understand how a Language Model is/should be trained, 
so that it can later be (re-)used in a transformer architecture model.
  
The idea I'm currently working on, is a sub task of NLG (natural language generation), where 
I may (or may not) do some summarization, generation, translation and/or classification tasks.

This paper discusses the main ideas which were also later used and refined in the BERT(arxiv:1810.04805), GPT and GPT-2 papers. 
I'll work through this paper first, mainly to keep the learning curve reasonable. (But maybe I've to go back until the TagLM-paper(arxiv:1705.00108), which is Pre-ELMo.)

But maybe it is enough to read the ELMo paper(arxiv:1802.05365), which describes some more recent ideas.

# Content

